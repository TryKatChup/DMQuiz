In Feature Selection,what is the Principal Component Analysis?
A. A mathematical technique used to transform non numeric attributes into numeric attributes
B. A mathematical technique used to transform a set of numeric attributes into a smaller set of numeric attributes which capture most of the variability in data
C. A heuristic technique used to find a subset of the attributes which produces the same classifier
D. A mathematical technique used to find the principal attributes which determine the classification process
B

A Decision Tree is...
A. A tree-structured plan of tests on single attributes to forecast the cluster 
B. A tree-structured plan of tests on single attributes to forecast the target
C. A tree-structured plan of tests on single attributes to obtain the maximum purity of a node
D. A tree-structured plan of tests on multiple attributes to forecast the target
B


In data preprocessing, which of the following are NOT the objectives of the Aggregation of attributes?
A. Obtain a more detailed description of data
B. Obtain a less detailed scale
C. Reduce the variability of data
D. Reduce the number of attributes or distinct values
A

In order to reduce the dimensionality of a dataset, which is the advantage of Multi Dimensional Scaling (MDS), with respect to Principal Component Analysis (PCA)
A. MDS requires less computational power
B. MDS can be used also with categorical data, provided that the matrix of the distance is available, while PCA is limited to vector spaces
C. MDS can be used with categorical data after a transformation in a vector space
D. MDS can work on any kind of data, while PCA is limited to categorical data
B

What is the main purpose of Smoothing in Bayesian classification?
A. Classifying an object containing attribute values which are missing from some classes in the training set
B. Dealing with missing values
C. Reduce the variability of the data
D. Classifying an object containing attribute values which are missing from some classes in the test set
A

Consider the transactional dataset below. Which is the Confidence of the rule B -> E ?~nID Items~n1 A,B,C~n2 A,B,D~n3 B,D,E~n4 C,D~n5 A,C,D,E
A. 100%
B. 50%
C. 33%
D. 20%
C

In a Decision Tree for classification, what is a Leaf Node?
A. A node which assigns a class value to the objects passing the tests on the path from the root to the node itself
B. A node where all the objects belong to the same class
C. A node which allows classification without errors
D. A node which assigns a class value only by majority of examples
A

Name the following rule evaluation formula: sup(AuC) - sup(A)sup(C)
A. Leverage
B. Lift
C. Confidence
D. Conviction
A

Name the following rule evaluation formula: conf(A->C)/sup(C)
A. Leverage
B. Lift
C. Confidence
D. Conviction
B

Name the following rule evaluation formula: sup(A->C)/sup(A)
A. Leverage
B. Lift
C. Confidence
D. Conviction
C

Name the following rule evaluation formula: [1- sup(C)]/[1-conf(A->C)]
A. Leverage
B. Lift
C. Confidence
D. Conviction
D

Which of the statements below best describes the strategy of Apriori in finding the frequent itemsets?
A. Evaluation of the support of the itemsets in an order such that uninteresting parts of the search space are considered only at the end of the execution
B. Evaluation of the confidence of the itemsets in an order such that uninteresting parts of the search space are pruned as soon as possible 
C. Evaluation of the support of the itemsets in an order such that uninteresting parts of the search space are pruned as soon as possible
D. Evaluation of the support of the itemsets in an order such that the interesting parts of the search space are pruned as soon as possible
C

What is the Gini Index?
A. A measure of the Entropy of a dataset
B. An accuracy measure of a dataset alternative to the Information Gain and to the Misclassification Index
C. An impurity measure of a dataset alternative to the Information Gain and to the Misclassification Index
D. An impurity measure of a dataset alternative to Overfitting and Underfitting
C

What measure is maximised by the Expectation Maximisation algorithm for clustering? 
A. The likelihood of a class label, given the attributes of the example
B. The likelihood of an attribute, given the class label
C. The likelihood of an example
D. The support of a class
A

Given the following definitions: TP = True Positives; TN = True Negatives; FP = False Positives; FN = False Negatives. Which of the formulas below computes the accuracy of a binary classifier?
A. TN / (TN + FP) 
B. (TP + TN) / (TP + FP + TN + FN)
C. TP / (TP + FN)
D. TP / (TP + FP)
B

In data preprocessing, which of the following IS NOT an objective of the Aggregation of attributes
A. Reduce the variability of data
B. Obtain a less detailed scale
C. Reduce the number of attributes or objects
D. Obtain a more detailed description of data
D

Which of the follwing is a strength of the clustering algorithm DBSCAN?
A. Requires to set the number of clusters as a parameter 
B. Ability to find cluster with concavities and separate outliers from regular data
C. Ability to find clusters regardless of density
D. Very fast by computation
B

Which of the following statements regarding the discovery of association rules is true?
A. The confidence of a rule can't be computed starting from the supports of the itemsets 
B. The support of an itemset is anti-monotonic with respect to the composition of the itemset
C. The support of a rule can be computed given the confidence of the rule
D. The confidence of an itemset is anti-monotonic with respect to the composition of the itemset
B

For this type of data choose the best suited distance function: Vector space with real values
A. Euclidean distance
B. Cosine distance
C. Manhattan distance
D. Jaccard coefficient
A

For this type of data choose the best suited distance function: Vectors of terms representing documents
A. Euclidean distance
B. Cosine distance
C. Manhattan distance
D. Jaccard coefficient
B

For this type of data choose the best suited distance function: High dimensional spaces
A. Euclidean distance
B. Cosine distance
C. Manhattan distance
D. Jaccard coefficient
C

For this type of data choose the best suited distance function: Boolean data
A. Euclidean distance
B. Cosine distance
C. Manhattan distance
D. Jaccard coefficient
D

Which of the following IS NOT a strength point of DBSCAN with respect to K-Means
A. The Robustness with respect to outliers
B. The Effectiveness, even in presence of ùòØùò∞ùò™ùò¥ùò¶
C. The Effectiveness even if there are clusters with non-convex shape
D. The Efficiency even in large datasets
D

Which is the effect of the Curse of Dimensionality
A. When the number of dimensions increases the results tend to be prone to overfitting 
B. When the number of dimensions increases the euclidean distance becomes less effective to discriminate between points in the space
C. When the number of dimensions increases the computing power necessary to compute the distances becomes too high
D. When the number of dimensions increases the classifiers cannot be correctly tuned
B

Which is different from the others?
A. Expectation Maximisation
B. Decision Tree
C. K-means
D. Dbscan
B

In a Neural Network, what is the Backpropagation?
A. The technique used to adjust the node weights according to the difference between the desired output and the output generated by the network
B. The technique used to adjust the weights limiting the probability of overfitting
C. The technique used to adjust the output according to the difference between the desired weights and the actual weights
D. The technique used to adjust the connection weights according to the difference between the desired output and the output generated by the network
D

Which is different from the others?
A. Dbscan
B. SVM
C. Neural Network
D. Decision Tree
A

Which of the following IS NOT a property of a Metric distance function
A. Symmetry
B. Positive definiteness
C. Triangle inequality
D. Boundedness
D

Which of the statements below is FALSE?
A. K-mean always stops to a configuration which gives the minimum distortion for the chosen value of the number of clusters 
B. K-means is very sensitive to the initial assignment of the centers 
C. K-means is quite efficient even for large datasets 
D. Sometimes K-means stops to a configuration which does not give the minimum distortion for the chosen value of the number of clusters
A

Which of the following characteristic of data can reduce the effectiveness of K-Means?
A. Presence of values with high frequency
B. Presence of outliers
C. All the variables have the same distribution of values
D. All the variables have the same range of values
B

Which is the purpose of discretisation?
A. Increase the number of distinct values in an attribute, in order to put in evidence possible patterns and regularities
B. Reduce the number of distinct values in an attribute, in order to increase the efficiency of the computations
C. Reduce the number of distinct values in an attribute, in order to put in evidence possible patterns and regularities
D. Reduce the range of values of a numeric attribute, to make all the attributes more comparable
C

Given the following definitions: TP = True Positives; TN = True Negatives; FP = False Positives; FN = False Negatives. Which of the formulas below computes the Recall of a binary classifier?
A. TP / (TP + FN)
B. TN / (TN + FP)
C. TP / (TP + FP)
D. (TP + TN) / (TP + FP + TN + FN)
A

The Information Gain is used to
A. Select the attribute which maximises, for a given training set, the ability to predict the class value
B. Select the attribute which maximises, for a given test set, the ability to predict the class value
C. Select the attribute which maximises, for a given training set, the ability to predict all the other attribute values
D. Select the class with maximum probability
A

Which is the main reason for the MinMax Scaling (also known as "Rescaling") of attributes?
A. Change the distribution of the numeric attributes, in order to obtain gaussian distributions
B. Map all the nominal attributes to the same range in order to prevent the values with higher frequency from having prevailing influence
C. Remove abnormal values 
D. Map all the numeric attributes to the same range, in order to prevent attributes with higher range from having prevalent influence
D

Which of the following is a base hypothesis for a Bayesian classifier?
A. The attributes must be statistically independent inside each class
B. The attributes must have negative correlation
C. The attributes must be statistically independent
D. The attributes must have zero correlation
A

When developing a classifier, which of the following is a symptom of overfitting?
A. The error rate in the test set is much smaller than the error rate in the training set
B. The precision is much greater than the recall
C. The error rate in the test set is more than 30%
D. The error rate in the test set is much greater than the error rate in the training set
D

With reference to the total sum of squared errors and separation of a clustering scheme, which of the statements below is true?
A. They are two ways to measure the same thing
B. They are strictly correlated, if, changing the clustering scheme, one increases, then the other does the same
C. It is possible to optimise them (i.e. minimise SSE and maximise SSB) separately
D. They are strictly correlated, if, changing the clustering scheme, one increases, then the other decreases
D
